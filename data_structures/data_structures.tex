\documentclass[11pt,a4paper]{article}

\def\nyear {2025}
\def\nterm {Winter}
\def\nlecturer {}
\def\ncourse {Data Structures}

\input{../header.tex}



\begin{document}
\maketitle

% Insert cool image here

\newpage
\tableofcontents
\newpage

\section{Introduction}

\subsection{Abstract data types}

\begin{example}
	How can we make a data structure for $15,\!000$ students
	that allows insertion and access to students by their id
	in time complexity of $O(1)$.
\end{example}

Data structures appear everywhere.

\begin{definition}[Abstract data type]
    To be added.
\end{definition}

\begin{example}[Stack]
    A stack has multiple basic fuctionalities.
    \begin{itemize}
        \item \texttt{push(S,x)} - add \texttt x to the top of the stack.
        \item \texttt{top(S)} - access top of the stack. Return null
            in the stack is empty.
        \item \texttt{pop(S)} - deletes the element on the top of the
            stack. Does not affect an empty stack.
        \item \texttt{create()} - returns an empty stack.
        \item \texttt{is\_empty(S)} returns an empty stack.
    \end{itemize}
\end{example}

We can implement this abstract data structure using either
an array, or a list. Which is more efficient?

A clear disadventage of an array is that the size of the
stack is limited.

A disadventage of a list is that our memory is limited.

In the real world we use a list of arrays.
We define an array of size $N$ to implement the stack, and when
we reach the maximum capacity of the array, we link
a new array of size $N$.

\begin{exercise}
    What is the absract data type defined by the functions
    \begin{itemize}
        \item \texttt{create(c)} - returns an initial instance.
        \item \texttt{read(c)} - returns an integer.
        \item \texttt{read(create(c))} - returns \texttt 0.
        \item \texttt{increment(c)} - changes \texttt c such that 
            \texttt{read(increment(c) = read(c) + 1}.
    \end{itemize}
\end{exercise}

The surprising answer is a counter!
\begin{itemize}
    \item \texttt{create(c)} - allocates memory to an integer $c$.
    \item \texttt{read(c)} - returns \texttt c.
    \item \texttt{increment(c)} - changes \texttt c such that 
        \texttt{c = c + 1}.
\end{itemize}

But there is a problem since there could be overflow!
How can we solve this?

\begin{example}[Queue]
    A queue has multiple basic fuctionalities.
    \begin{itemize}
        \item \texttt{enqueue(Q,x)} - add $x$ to the end of the queue.
        \item \texttt{head(S)} - access start of the stack. Return null
            in the queue is empty.
        \item \texttt{dequeue(S)} - deletes the first element of the queue,
            and return the first element of the modified queue.
            Does not affect an empty queue.
        \item \texttt{create()} - returns an empty queue.
        \item \texttt{is\_empty($S$)} returns true if the queue is empty.
    \end{itemize}
\end{example}

We can implement a queue using an array, or a linked list.

In \texttt{c++} we can implement an abstract data type in multiple ways, then
by using
% \[
%     typedef ArrayBasedStack Stack;
%     typedef ListBasedStack Stack;
% \]

The quality of the implementation is affected by the time and space complexity
of the functions, and the simplicity of the implementation.

\subsection{Time complexity}

\begin{definition}[Run time]
    The run time of algorithm $A$ on input $x$ is the number of machine
    commands that the algorithm runs on the input.
    We denote it $\timeop_A(x)$.
\end{definition}
\begin{remark}
    We don't need to bother about the different in machines that execute
    the commands, and we also don't bother about the difference in run-time
    of different commands.
\end{remark}

Since we can't possibly write the run time for all inputs, we denote:
\[
  \t_A(n) = \max\set{\timeop_A(x) \mid |x| = n}.
\]

\begin{example}[For loop]
    Consider the following code:

    we have that
    \[
        n + 1 \le \t_A(n) \le c_1 \cdot n + c_2.
    \]
    Where $c_1$, $c_2$ are constants that depend on the the implementation
    of the specific programming language.
\end{example}

\begin{example}[check prime]
    Consider the following code:

    We have that
    \begin{align*}
        &\timeop(9888) = c_1 + c_2 \\
        &\timeop(9973) = 9971 \cdot c_1 + c_2 \\
        &\timeop(4) = 9971 \cdot c_1 + c_2 \\
        &\timeop(n) = 10^n \cdot c_1 + c_2
    \end{align*}
\end{example}
\begin{remark}
    Notice that we talk about the \emph{size} of the input and not the 
    \emph{value} of the input.
    Here $n$ is the ?????
\end{remark}

As we said earlier we don't really care about the constants.

We can think about $f(n)$ the function of the real time of the algorithm.

\begin{definition}
    Let $f(n)$, $g(n)$ be two positive functions defined on the natural
    numbers.
    We say that $f(n) \in O(g(n))$ if exist $c > 0$ and $n_0 > 0$ such
    that for all $n > n_0$ we have $f(n) \le c \cdot g(n)$.
\end{definition}

If $f(n) \in O(g(n))$ we usually denote $f(n) = O(g(n))$ and say that
$g(n)$ is an asymptotoic bound of $f(n)$.

\begin{proposition}
    Let $k$ be a constant. Then
    \[
        f(n) = \sum_{i=0}^{k} a_k n^k = O(n^k).
    \]
\end{proposition}
\begin{proof}
    Let $a_{\max} = \max\set{1,a_0,\dots,a_k}$.
    Then we have
    \[
        f(n) \le a_{\max}(1 + n + \dots + n^k) =
        a_{\max} \frac{n^{k+1}}{n-1} \le
        a_{\max} \frac{n^{k+1}}{n/2} = 2 a_{\max} n^k.
    \]
    Thus we can choose $n_0 = 2$ and $c = 2 a_{\max}$ and then
    $f(n) \le c \cdot n^k$ for all $n > n_0$.
\end{proof}

Here are a couple more examples:
\begin{align*}
    &f(n) = 10006 = O(1) \text{ (Constant)} \\
    &f(n) = 4n + 27 =O(n) \text{ (Linear)} \\
    &f(n) = n \log_2(n) + 3n^2 = O(n^2) \text{ (Polynomial)}
\end{align*}

\begin{proposition}
    For all $a,b > 1$ we have that
    \[
        f(n) = \log_a(n) = O(\log_b(n)).
    \]
\end{proposition}

A polynomial is stronger than any logarithm.

\begin{proof}
    For all $\epsilon > 0$ and $a > 1$ we have
    \[ f(n) = \log_a n = \frac{\epsilon \cdot}{a}. \]
\end{proof}

More examples are

\begin{align*}
    &f(n) = a^n = O(b^n) \text{ (Exponential)} \\
    &f(n) = n^k + a^n = O(a^n) \\
    &f(n) = n \cdot log_2 n = O(n \log n)
\end{align*}

We can also show that


\begin{align*}
    &f(n) = 3^n \neq O(2^n) \text{ (Exponential)} \\
    &f(n) = e^n \neq O(n^k) \\
    &f(n) = \log n \neq O(\log \log n)
\end{align*}

\begin{proof}
    Assume by contradiction that exist $c > 0$ and $n_0 \geq 0$ such that
    for all $n > n_0$ we have $3^n \le c \dot 2^n$.
    This implies that $\del{\frac 32}^n \le c$ for all $n > n_0$.
    But this is obviously a contradiction which completes the proof.
\end{proof}

\begin{proof}
    Let $k > 0$.
    We know from analysis that
    \[
        \lim_{x \to \infty} \frac{x^k}{e^x} = 0
    \]
    by L'hopital which imlies that $e^n \neq O(n^k)$ for all $k > 0$
    as wanted.
\end{proof}

\begin{proof}
    Similarly we have that
    \[
        \lim_{n \to \infty} \frac{\log \log n}{\log n} = 0
    \]
    which completes the proof.
\end{proof}

\begin{example}
    Summing elements of an arary of length $n$ is $O(n)$.
\end{example}

\begin{example}[Multiplying matrices]
    When multiplying matrices of order $m \times m$ the complexity
    of the algorithm is $O(m)$.
    But $m$ is not the size of the input.
    Sinc the input is of $2$ matrices it is equal $2m^2$.
    Therefore
    \[
        T(n) = O(m^3) = O\del{\frac{n^{\frac 32}}{2^{\frac 32}}} =
        O(n^{\frac 32})
    \]
\end{example}

\begin{example}[Binary search]
    In a binary search we have that the algorithm executes some commands
    on input of size $n$, and then it executes the same commands on input
    of size
\end{example}

\begin{example}[Weird loops]
    Consider the following code:
    Roughly we have that
    \[
        T(n) \le n (n - 1) = O(n^2).
    \]
    But more precisely we have that
\end{example}

\begin{definition}
    Let $f(n)$, $g(n)$ be two positive functions defined on the natural
    numbers.
    We say that $f(n) \in \Omega(g(n))$ if exist $c > 0$ and $n_0 > 0$ such
    that for all $n > n_0$ we have $f(n) \geq c \cdot g(n)$.
\end{definition}

\begin{remark}
  \[
    g(n) = O(f(n)) \iff
    f(n) = \Omega(g(n))
  \]
\end{remark}

\begin{definition}
    Let $f(n)$, $g(n)$ be two positive functions defined on the natural
    numbers.
    We say that $f(n) = \Theta(g(n))$ if 
    $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.
\end{definition}

\begin{example}
  We have that $H_n = \Theta(\ln n)$ because
  \[
    \ln(n + 1) = \int{1}^{n+1}
  \]
\end{example}

\begin{definition}
    Let $f(n)$, $g(n)$ be two positive functions defined on the natural
    numbers.
    We say that $f(n) = o(g(n))$ if 
    \[
      \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0.
    \]
\end{definition}

\begin{example}
\end{example}

Note that asymptotic complexity isn't always what we want to optimize.

\begin{example}
  Suppose algorithm $A$ has a running time of $\t_A(n) = 100n$, and
  algorithm $B$ has a running time of $\t_B(n) = 5 n \log_2 n$.
  Asympotically algorithm $A$ is better because it is linear, but
  for algorithms with $n < 2^{20}$ algorithm $B$ is more efficient.
\end{example}
\begin{example}
  We would rather have $\t_A(n) = n^2$ than $\t_B(n) = 10^{160}$
  even though $B = O(1)$.
\end{example}


\section{Arrays, matrices, sparse matrices and linked lists}
\subsection{Arrays}

\begin{example}[Array]
    An array has multiple basic fuctionalities.
    \begin{itemize}
        \item \texttt{create(type,I)} - returns an array of elements of type
          \texttt{type} with the index set \texttt I.
        \item \texttt{store(A,i,e)} - stores \texttt e of type \texttt{type}
          with index \texttt i to the array.
          This usually looks like \texttt{A[i] = e}.
        \item \texttt{get(A,i)} - returnes the element in index \texttt i.
          This usually looks like \texttt{A[i]}.
    \end{itemize}
\end{example}

All the values in the array are of the same type.
Notice that we didn't define what happens if we try to store an element
of a different type, or what to return when we use get on an uninitialized
value.

Here are some examples of implementations.

\begin{example}[Continuous memory block]
  This is the most trivial implementation.
  In this way we have
  \begin{itemize}
    \item The functions \texttt{store} and \texttt{get} in $O(1)$
    \item TO BE CN
  \end{itemize}
\end{example}

\begin{example}[Multiple continuous memory block of size $n$]
  To implemet an array of size $m$ we need to allocate $k = \ceil{m / n}$
  blocks.
  In this way we only need to keep a pointer array to the start of each block.
  To access an element in this kind of array, we can use basic arithmetics.
  For example, if $n = 6$ and $i = 16$ the address of $A[16]$ is
  \[
    base\del{\floor{\frac{16}{6}}} + 16 \% 6 = base\del{2} + 4
  \]
  where $base$ is the pointer array to the start of the blocks.
  The address computation time is still $O(1)$.
\end{example}

\begin{example}[$2d$-array]
  In a $2d$-array $A[i][j]$ each row $A[i]$ is a $1d$-array of size $n$.
  To find the element in $A[i][j]$ we can calculate
  \[
    base + i \cdot n + j
  \]
  where $base$ is the beginning of the array.
\end{example}

\begin{example}[$n$-dimensional array]
  An $n$-dimensional array is very similar to a $2$-dimensional array.
  To find the address of $A[i_d]\dots[i_1]$ we get
  \[
    base + i_d \cdot n_{d-1} \cdots n_1 + i_{d-1} n_{d-2} \cdots n_1 +
    \cdots + i_1
  \]
  The number of operations in this calculations is $O(n^2)$.
  \begin{question}
    Is there a way to optimize this calculation.
  \end{question}
  \begin{answer}
    Yes! using Horner's method we can do the same computation in a linear
    amount of operations.
  \end{answer}
\end{example}

\begin{remark}
  In general, when scanning the elements of an $n$-dimensional array
  it is better to scan them by rows and not by columns because of 
  \emph{locality}. It is easier for the computer to access memory that
  is close to the previously accessed memory.
  Reading memory is more expensive than than calculting an operation in the GPU.
\end{remark}

\subsection{Initializing an array in \texorpdfstring{$O(1)$}{O(1)}}
In the trivial way, initializing an array of size $n$ takes $O(n)$ operations.
We want to initialze an array in $O(1)$, for this, we will use more memory.

\begin{example}[First try]
  We will also allocate an auxillary array of size $n$.
  Each time we store a value in index $i$ we add index $i$ to the top
  of the auxillary array.
  In this way, before trying to access to array, we check if index $i$
  is in the auxillary array, and if not, return $0$ (if we wanted to initialize
  the array in $0$).

  This implementation is problamatic because when accessing the element
  in index $i$ we first need to scan the auxillary array which takes
  $O(n)$ operations.
\end{example}

\begin{example}[Second try]
  This time, we will hold three arrays.
  An array for values, shortcuts, and indexes with value.
  In this way, when accessing an element in the index $i$, we first
  check if the shortcuts array in index $i$.
\end{example}

To implement this we first define the function

% int  is_assigned(int i) {
% return ( B[i] < top  &&  B[i] >= 0  &&  C[B[i]] = = i );  
% }
%
% And then implement the rest of the functions by
%
% top = 0;
% constant = const;
%
% if  (is_assigned(i)) 
%          return  A[i];
% else 
%          return constant;
%
% f  (!is_assigned(i)) {
%       C[top] = i ;
%       B[i] = top ;
%       top = top +1;}
% A[i]= e;

\begin{remark}
  After $top \geq n$ we don't have to use the auxillary arrays anymore.
\end{remark}

Merits
\begin{itemize}
  \item Asymptotically this is a great solution.
  \item Practically
\end{itemize}

Demertis
\begin{itemize}
  \item Takes three times more space.
  \item The initialization time is later backstabbed by the new implementations
    of \texttt{get} and \texttt{store} (amortized analysis).
\end{itemize}

\subsection{Matrices}

\begin{example}[Symmetrical matrices]
  Suppose we have a matrix like
  \[\begin{pmatrix}
    1 & 2 & 4 & 7 & 12 \\
    2 & 3 & 5 & 8 & 1 \\
  \end{pmatrix}\]
  We can just hold some

  To calculate the address we get
\end{example}

\begin{example}[Sparse matrix]
  A sparse matrix is a matrix where most elements are $0$ (or some other
  constant).

  \begin{definition}[Sparse matrix]
    If $M_1,\dots,M_n$ is a sequence of matrices where the number of elements
    different from some constant $c$ is $o(m(n))$ where $m(n)$ is the number
    of elements in the $n$th matrix then it is a sequence of sparse matrices.
  \end{definition}

  \begin{example}
    Diagonal matrices are sparse matrices.
  \end{example}

  \begin{remark}
    In sparse matrices of order $n \times n$ the number of elements different
    than $c$ is $o(n^2)$.
  \end{remark}

  One way to represent these matrices is by a lexicographical list
  of triples $(i,j,A_{i,j})$ that represent the values different than $c$
  in the matrix.
  On one hand, we lose \texttt{get} in constant time.
  On the other hand, in this way allocation is way more efficient.
  Addition and multiplication of sparse matrices are way more efficient.

  \begin{example}[Addition of sparse matrices]
    Addition in usual matrices of order $n \times n$ takes $o(n^2)$ time.
    When adding sparse matrices in this repersentation, adding them is as
    bad as combining their lists (of elemenets different from $c$).
    Suppose their lists are of size $m$ and $k$, the time complexity is
    now only $o(m + k)$.
  \end{example}
\end{example}

\subsection{Linked lists}

Recall that a linked list

% % Add recalling
%
% The implementation looks something like this:
%
% init(head)
%
% void  init (NODE  *head){
% * head = NULL;
% }
%
% find(x,head)
%
% NODE * find (DATA_TYPE x, NODE  *head){
% NODE *t;
% t = head;
% while (t!=NULL && t → info != x)
%  t = t → next ;
% return t;
% }
%
% insert(t,x)
%
% int  insert ( NODE  *t,  DATA_TYPE x){
% NODE *p;
% if (t == NULL) return 0;
% p = (NODE *) malloc (sizeof (NODE));
% p → info = x;
% p → next = t → next ;
% t → next  = p ;
% return 1;
% }
%
% delete(t)
%
% delete ( NODE  *t){
% NODE  * temp ;
% temp = t → next;
% t → next = temp → next ;
% free (temp) ;
% }
%
% delete_first(r)
%
% int delete_first ( NODE  **p_head){
% NODE  * temp ;
% if (*p_head == NULL)  return 0 ;           
% temp = *p_head;
% *p_head = temp → next ;
% free (temp) ;
% return 1;            /* success code */
% }

There are different kinds of linked lists.

\begin{example}[Dummy node linked list]
  A dummy node linked list is just like a standard linked list,
  with an additional node at the beginning without any meaningful value
  to make deletion more easily
\end{example}

\begin{example}[Cyclic linked list]
  A linked list where the last element is pointing to the head node.
  Some merits are
  \begin{itemize}
    \item We can reach any element from any element.
    \item We can concatenate lists in constant time, because we have constant
      time access to the pointer of the last node.
  \end{itemize}
\end{example}

\begin{example}[Doubly linked list]
  In a doubly linked list we have not only pointers to the next nodes,
  but also the the previous nodes.
  Mertis:
  \begin{itemize}
    \item We can reach any element from any element.
    \item Given access to a node $t$ we can delete it in constant time.
  \end{itemize}
\end{example}

There is also a tricky deletion in standard linked lists.

TO BE CONTINUED

\subsection{Extra}

Parse polynomials.

\begin{definition}
  DEFINE
\end{definition}

Instead of representing sparse polynomials in an array of size of the order
of the polynomial, we can represent the nonzero coefficients in a linked list

MORE IN PRESENTATION

































\end{document}
